{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoted data saved to /home/safi/sanjay/PIB/mapping/combined_scraped_prids.csv\n"
     ]
    }
   ],
   "source": [
    "# CODE TO CONSOLIDATE ALL EXTRACTED PRIDS INTO ONE .csv FILE\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing CSV files\n",
    "directory = '/home/safi/sanjay/PIB/scraped_prids'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Read all CSV files and concatenate them into a single DataFrame\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "\n",
    "    # Add a check to skip empty files\n",
    "    if os.path.getsize(file_path) > 0:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Skipped empty file: {file_path}\")\n",
    "\n",
    "# Merge all DataFrames based on 'Main_PRID'\n",
    "if dfs:\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Pivot the DataFrame based on 'Main_PRID' and 'Extracted_Language'\n",
    "    pivot_table = merged_df.pivot_table(index='Main_PRID', columns='Extracted_Language', values='Extracted_PRID', aggfunc='first')\n",
    "\n",
    "    # Reset index to make 'Main_PRID' a column\n",
    "    pivot_table.reset_index(inplace=True)\n",
    "\n",
    "    # Save the pivoted data to a new CSV file\n",
    "    output_file = '/home/safi/sanjay/PIB/mapper/combined_scraped_prids.csv'\n",
    "    pivot_table.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Pivoted data saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data saved to /home/safi/sanjay/PIB/mapping/initial_map.csv\n"
     ]
    }
   ],
   "source": [
    "# CODE TO MAP THE PRIDS WITH THE FILE PATHS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "combined_pib_data_path = '/home/safi/sanjay/PIB/mapper/combined_scraped_prids.csv'\n",
    "path_prid_path = '/home/safi/sanjay/PIB/scraper/path_prid.csv'\n",
    "\n",
    "combined_pib_data = pd.read_csv(combined_pib_data_path)\n",
    "path_prid = pd.read_csv(path_prid_path)\n",
    "\n",
    "# Merge the two dataframes on the 'Main_PRID' column\n",
    "merged_data = pd.merge(combined_pib_data, path_prid, left_on='Main_PRID', right_on='PRID', how='left')\n",
    "\n",
    "# Create a list to store the mappings\n",
    "mappings = []\n",
    "\n",
    "# Iterate through the merged data and populate the mappings list\n",
    "for index, row in merged_data.iterrows():\n",
    "    prid = row['Main_PRID']\n",
    "    main_file_path = row['File Path']\n",
    "    \n",
    "    # Iterate through languages and their columns in the dataframe\n",
    "    for lang in combined_pib_data.columns[2:]:\n",
    "        lang_prid = row[lang]\n",
    "        if pd.notnull(lang_prid):  # Check if the language PRID exists\n",
    "            lang_file_paths = path_prid[path_prid['PRID'] == lang_prid]['File Path'].values\n",
    "            if len(lang_file_paths) > 0:\n",
    "                lang_file_path = lang_file_paths[0]  # Take the first matching path\n",
    "\n",
    "                # Create a dictionary for each mapping\n",
    "                mapping = {\n",
    "                    'Main_PRID': prid,\n",
    "                    'Main_File_Path': main_file_path,\n",
    "                    'Language': lang,\n",
    "                    'Language_PRID': lang_prid,\n",
    "                    'Language_File_Path': lang_file_path,\n",
    "                }\n",
    "\n",
    "                mappings.append(mapping)\n",
    "\n",
    "# Convert the mappings list to a pandas DataFrame\n",
    "mappings_df = pd.DataFrame(mappings)\n",
    "\n",
    "# Write the mappings to a new CSV file\n",
    "output_csv_path = '/home/safi/sanjay/PIB/mapper/initial_map.csv'\n",
    "mappings_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"CSV data saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN .py file of this for bigger ranges\n",
    "# FINAL CODE TO EXTRACT TEXT FROM THE FILE PATH AND STORE IN SEPERATE JSON\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Function to extract text from JSON file\n",
    "def extract_text(file_path):\n",
    "    if pd.notnull(file_path) and os.path.exists(str(file_path)):\n",
    "        with open(str(file_path), 'r') as json_file:\n",
    "            content = json.load(json_file)\n",
    "            return content.get('text', '')\n",
    "    else:\n",
    "        return ''  # Return an empty string if file path is missing or file doesn't exist\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = '/home/mtech_22/sanjay/PIB/initial_map_modified.csv'\n",
    "output_directory = '/home/mtech_22/sanjay/pib_final'\n",
    "zip_file_name = '/home/mtech_22/sanjay/pib_final.zip'\n",
    "\n",
    "# Read the entire CSV file (remove nrows parameter)\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a temporary directory to store JSON files\n",
    "temp_directory = '/home/mtech_22/sanjay/temp_json'\n",
    "os.makedirs(temp_directory, exist_ok=True)\n",
    "\n",
    "# Group data by 'Main_PRID' column\n",
    "grouped_data = data.groupby('Main_PRID')\n",
    "\n",
    "# Create separate JSON files for each 'Main_PRID' containing all language texts\n",
    "for group_name, group_df in grouped_data:\n",
    "    json_data = {'Main_PRID': group_name, 'Main_PRID_Language': '', 'Main_PRID_Text': '', 'Languages': []}\n",
    "\n",
    "    # Get unique Language_PRIDs in the group\n",
    "    unique_language_prids = group_df['Language_PRID'].unique()\n",
    "\n",
    "    # Extract text for each language within the group\n",
    "    for language_prid in unique_language_prids:\n",
    "        language_df = group_df[group_df['Language_PRID'] == language_prid].iloc[0]  # Get the first row for the language\n",
    "        language_details = {\n",
    "            'Language': language_df['Language'],\n",
    "            'Language_PRID': language_df['Language_PRID'],\n",
    "            'Text': extract_text(language_df['Language_File_Path'])  # Extract text using the provided file path\n",
    "        }\n",
    "        json_data['Languages'].append(language_details)\n",
    "\n",
    "    # Extract and add text associated with Main_PRID\n",
    "    main_prid_text = extract_text(group_df['Main_File_Path'].iloc[0])\n",
    "    json_data['Main_PRID_Language'] = ''  # Add an empty element for Main_PRID_Language\n",
    "    json_data['Main_PRID_Text'] = main_prid_text\n",
    "\n",
    "    # Write the JSON data to a separate file for the Main_PRID in the temporary directory\n",
    "    output_file_path = os.path.join(temp_directory, f\"data_{group_name}.json\")\n",
    "    with open(output_file_path, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "# Create a ZIP file containing all JSON files\n",
    "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
    "    for foldername, subfolders, filenames in os.walk(temp_directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, temp_directory))\n",
    "\n",
    "# Remove the temporary directory and its contents\n",
    "shutil.rmtree(temp_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO UNZIP THE FILE FOR \"langdetect\"\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(zip_path, extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zip_path = \"/home/mtech_22/sanjay/finally.zip\"\n",
    "    extract_path = \"/home/mtech_22/sanjay/unzipped_files\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    unzip_file(zip_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING \"langdetect\" TO GET THE MAIN_PRID LANGUAGE AND UPDATE IN THE SAME .json FILE\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "def process_json_file(json_data):\n",
    "    main_prid_text = json_data.get(\"Main_PRID_Text\", \"\")\n",
    "    language = detect_language(main_prid_text)\n",
    "    \n",
    "    # Check if \"Main_PRID_Language\" key is present\n",
    "    if \"Main_PRID_Language\" in json_data:\n",
    "        json_data[\"Main_PRID_Language\"] = language\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        # Attempt to detect language\n",
    "        lang_code = detect(text)\n",
    "        \n",
    "        # Map language code to language name\n",
    "        lang_name = lang_code_to_name(lang_code)\n",
    "        \n",
    "        return lang_name\n",
    "    except LangDetectException:\n",
    "        # Handle exceptions, e.g., if text is too short\n",
    "        return \"Unknown\"\n",
    "\n",
    "def lang_code_to_name(code):\n",
    "    # Language mapping for Indian languages\n",
    "    indian_language_mapping = {\n",
    "        \"en\": \"English\",\n",
    "        \"bn\": \"Bengali\",\n",
    "        \"gu\": \"Gujarati\",\n",
    "        \"hi\": \"Hindi\",\n",
    "        \"kn\": \"Kannada\",\n",
    "        \"ml\": \"Malayalam\",\n",
    "        \"mr\": \"Marathi\",\n",
    "        \"ne\": \"Nepali\",\n",
    "        \"pa\": \"Punjabi\",\n",
    "        \"ta\": \"Tamil\",\n",
    "        \"te\": \"Telugu\",\n",
    "        \"ur\": \"Urdu\",\n",
    "    }\n",
    "    return indian_language_mapping.get(code, \"Unknown\")\n",
    "\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                    json_data = json.load(json_file)\n",
    "                    process_json_file(json_data)\n",
    "                    \n",
    "                    # Save the modified JSON back to the file\n",
    "                    with open(file_path, 'w', encoding='utf-8') as updated_json_file:\n",
    "                        json.dump(json_data, updated_json_file, indent=4, ensure_ascii=False)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"/home/mtech_22/sanjay/unzipped_files\"\n",
    "\n",
    "    process_directory(directory_path)\n",
    "    print(\"Language detection and modification completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
